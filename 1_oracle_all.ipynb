{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e28ee6",
   "metadata": {},
   "source": [
    "# ML Model for All 500,000 Grids (The Oracle Builder)\n",
    "\n",
    "* Refinement: Do not just build one ML model; focus on building four high-accuracy models (one for each advisor) and using an AutoML tool (like AutoGluon or a deep ensemble) for robustness.\n",
    "* Key Action: Feature Engineering is Paramount. Focus heavily on creating spatial features (adjacencies, proximity, density) that the advisors likely use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f72db57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Grids shape: (500000, 7, 7)\n",
      "Ratings shape: (500000, 4)\n",
      "Available ratings per advisor: [5000 5000 5000 5000]\n"
     ]
    }
   ],
   "source": [
    "# Setup and Data Loading\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Clone repository if not exists and load data\n",
    "if not os.path.exists('2155-Challenge-Problem-2'):\n",
    "    import subprocess\n",
    "    print(\"Cloning repository...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/Lyleregenwetter/2155-Challenge-Problem-2'], \n",
    "                   check=True, cwd='.')\n",
    "    print(\"Repository cloned!\")\n",
    "\n",
    "# Change to the repository directory and load data\n",
    "os.chdir('2155-Challenge-Problem-2')\n",
    "\n",
    "# Import utilities\n",
    "from utils_public import load_grids, plot_n_grids\n",
    "\n",
    "print(\"Loading data...\")\n",
    "grids = load_grids()\n",
    "ratings = np.load(\"datasets/scores.npy\")\n",
    "\n",
    "print(f\"Grids shape: {grids.shape}\")\n",
    "print(f\"Ratings shape: {ratings.shape}\")\n",
    "print(f\"Available ratings per advisor: {(~np.isnan(ratings)).sum(axis=0)}\")\n",
    "\n",
    "advisor_names = [\"Wellness\", \"Tax\", \"Transportation\", \"Business\"]\n",
    "\n",
    "# Change back to parent directory for saving results\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8b66e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking PyTorch environment...\n",
      "‚úÖ PyTorch available - Device: cpu\n",
      "   üíª Using CPU for CNN training\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch availability for CNN models\n",
    "print(\"üîç Checking PyTorch environment...\")\n",
    "try:\n",
    "    import torch\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"‚úÖ PyTorch available - Device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   üöÄ CUDA detected: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(f\"   üíª Using CPU for CNN training\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not found - CNN models will not work\")\n",
    "    print(\"   Install with: pip install torch torchvision\")\n",
    "\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5671fe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading pre-trained Oracle models from pickle files...\n",
      "üìÅ Loading Business Oracle from data/models/business_oracle_model.pkl...\n",
      "‚úÖ Business Oracle loaded successfully!\n",
      "üìÅ Loading Wellness Oracle from data/models/wellness_oracle_model.pkl...\n",
      "üî• PyTorch available, using device: cpu\n",
      "‚ùå Failed to load Wellness Oracle: 'model'\n",
      "   Error details: KeyError\n",
      "üìÅ Loading Tax Oracle from data/models/tax_oracle_model.pkl...\n",
      "‚úÖ Tax Oracle loaded successfully!\n",
      "üìÅ Loading Transportation Oracle from data/models/transportation_oracle_model.pkl...\n",
      "üî• PyTorch available, using device: cpu\n",
      "‚ùå Failed to load Transportation Oracle: 'model'\n",
      "   Error details: KeyError\n",
      "\n",
      "üìä Successfully loaded 2 pre-trained models:\n",
      "   ‚úì Business Oracle (CNN)\n",
      "   ‚úì Tax Oracle (CNN)\n",
      "‚úÖ Pre-trained Oracle predictor ready!\n",
      "üéØ Can predict for 2 advisors without training\n",
      "üß† Using CNN models for: Wellness, Transportation\n",
      "üìä Using traditional models for: Business, Tax\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Oracle models from pickle files\n",
    "import os\n",
    "from oracle import CNNTransportationOracle, CNNWellnessOracle\n",
    "from oracle import BusinessOracle, TaxOracle  # Keep traditional ones for Business and Tax\n",
    "\n",
    "print(\"üîÑ Loading pre-trained Oracle models from pickle files...\")\n",
    "\n",
    "# Define available oracle classes\n",
    "oracle_classes = {\n",
    "    'Business': BusinessOracle,\n",
    "    'Wellness': CNNWellnessOracle,  # Use CNN version\n",
    "    'Tax': TaxOracle,\n",
    "    'Transportation': CNNTransportationOracle  # Use CNN version\n",
    "}\n",
    "\n",
    "# Initialize oracles\n",
    "oracle_files = {\n",
    "    'Business': 'data/models/business_oracle_model.pkl',\n",
    "    'Wellness': 'data/models/wellness_oracle_model.pkl',  # Updated filename\n",
    "    'Tax': 'data/models/tax_oracle_model.pkl',\n",
    "    'Transportation': 'data/models/transportation_oracle_model.pkl'  # Updated filename\n",
    "}\n",
    "\n",
    "# Load available pre-trained models\n",
    "loaded_models = {}\n",
    "for advisor_name, filename in oracle_files.items():\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"üìÅ Loading {advisor_name} Oracle from {filename}...\")\n",
    "        try:\n",
    "            # Create oracle instance using the appropriate class\n",
    "            oracle_class = oracle_classes[advisor_name]\n",
    "            oracle = oracle_class()\n",
    "            oracle.load_model(filename)\n",
    "            \n",
    "            loaded_models[advisor_name] = oracle\n",
    "            print(f\"‚úÖ {advisor_name} Oracle loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {advisor_name} Oracle: {e}\")\n",
    "            print(f\"   Error details: {type(e).__name__}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {filename} not found - will need fallback for {advisor_name}\")\n",
    "\n",
    "print(f\"\\nüìä Successfully loaded {len(loaded_models)} pre-trained models:\")\n",
    "for advisor_name in loaded_models.keys():\n",
    "    oracle = loaded_models[advisor_name]\n",
    "    oracle_type = \"CNN\" if hasattr(oracle, 'model') else \"Traditional\"\n",
    "    print(f\"   ‚úì {advisor_name} Oracle ({oracle_type})\")\n",
    "\n",
    "# Create a combined predictor class for easy use\n",
    "class PreTrainedOraclePredictor:\n",
    "    def __init__(self, loaded_models):\n",
    "        self.loaded_models = loaded_models\n",
    "        self.advisor_names = [\"Wellness\", \"Tax\", \"Transportation\", \"Business\"]\n",
    "        \n",
    "    def predict_advisor(self, grids, advisor_idx):\n",
    "        \"\"\"Predict using pre-trained model for specific advisor\"\"\"\n",
    "        advisor_name = self.advisor_names[advisor_idx]\n",
    "        \n",
    "        if advisor_name in self.loaded_models:\n",
    "            oracle = self.loaded_models[advisor_name]\n",
    "            print(f\"üîÆ Predicting with {advisor_name} Oracle...\")\n",
    "            return oracle.predict(grids)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No pre-trained model for {advisor_name}, returning zeros\")\n",
    "            return np.zeros(len(grids))\n",
    "    \n",
    "    def predict_all_advisors(self, grids):\n",
    "        \"\"\"Predict for all advisors using available pre-trained models\"\"\"\n",
    "        print(f\"üöÄ Generating predictions for all advisors...\")\n",
    "        predictions = []\n",
    "        for advisor_idx in range(4):\n",
    "            advisor_predictions = self.predict_advisor(grids, advisor_idx)\n",
    "            predictions.append(advisor_predictions)\n",
    "        return np.stack(predictions).T\n",
    "\n",
    "# Initialize the combined predictor\n",
    "oracle_predictor = PreTrainedOraclePredictor(loaded_models)\n",
    "print(f\"‚úÖ Pre-trained Oracle predictor ready!\")\n",
    "print(f\"üéØ Can predict for {len(loaded_models)} advisors without training\")\n",
    "print(f\"üß† Using CNN models for: Wellness, Transportation\")\n",
    "print(f\"üìä Using traditional models for: Business, Tax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15efd227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Checking for missing models and creating fallbacks...\n",
      "üöß Creating fallback CNN model for Wellness...\n",
      "üî• PyTorch available, using device: cpu\n",
      "   üìö Quick training on subset for Wellness...\n",
      "\n",
      "============================================================\n",
      "Training PyTorch CNN Transportation Oracle\n",
      "============================================================\n",
      "Available training samples: 1000\n",
      "Training parameters:\n",
      "  - Epochs: 5\n",
      "  - Batch size: 128\n",
      "  - Test size: 0.2\n",
      "  - Learning rate: 0.0003\n",
      "  - Model type: standard\n",
      "  - Device: cpu\n",
      "üî• Using PyTorch CNN approach for spatial pattern recognition...\n",
      "üèóÔ∏è CNN Architecture:\n",
      "CityCNN1(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "  (head): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Dropout(p=0.25, inplace=False)\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Dropout(p=0.25, inplace=False)\n",
      "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "üöÄ Training PyTorch CNN model...\n",
      "Epoch   1: Train Loss = 0.270356, Val Loss = 0.238076, LR = 3.00e-04\n",
      "\n",
      "üìä PyTorch CNN Performance:\n",
      "   Training R¬≤: 0.1354\n",
      "   Test R¬≤: 0.0824\n",
      "   ‚úÖ Wellness fallback model trained!\n",
      "üöß Creating fallback CNN model for Transportation...\n",
      "üî• PyTorch available, using device: cpu\n",
      "   üìö Quick training on subset for Transportation...\n",
      "\n",
      "============================================================\n",
      "Training PyTorch CNN Transportation Oracle\n",
      "============================================================\n",
      "Available training samples: 1000\n",
      "Training parameters:\n",
      "  - Epochs: 5\n",
      "  - Batch size: 128\n",
      "  - Test size: 0.2\n",
      "  - Learning rate: 0.0003\n",
      "  - Model type: standard\n",
      "  - Device: cpu\n",
      "üî• Using PyTorch CNN approach for spatial pattern recognition...\n",
      "üèóÔ∏è CNN Architecture:\n",
      "CityCNN1(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "  (head): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Dropout(p=0.25, inplace=False)\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Dropout(p=0.25, inplace=False)\n",
      "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "üöÄ Training PyTorch CNN model...\n",
      "Epoch   1: Train Loss = 0.360481, Val Loss = 0.469615, LR = 3.00e-04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m train_scores = ratings[train_mask, advisor_idx][:\u001b[32m1000\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Quick training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43moracle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_grids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m fallback_models[advisor_name] = oracle\n\u001b[32m     30\u001b[39m loaded_models[advisor_name] = oracle  \u001b[38;5;66;03m# Add to loaded models\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\2156_cp2\\oracle.py:932\u001b[39m, in \u001b[36mCNNTransportationOracle.fit_model\u001b[39m\u001b[34m(self, grids, ratings, epochs, batch_size, test_size, learning_rate, weight_decay, early_stopping_patience, use_augmentation, model_type, **model_params)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m    931\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m optimizer.step()\n\u001b[32m    935\u001b[39m train_loss += loss.item() * batch_x.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\anaconda3\\envs\\egg\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\anaconda3\\envs\\egg\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\anaconda3\\envs\\egg\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create fallback CNN models if pre-trained models don't exist\n",
    "fallback_models = {}\n",
    "\n",
    "print(\"\\nüîß Checking for missing models and creating fallbacks...\")\n",
    "\n",
    "for advisor_name in [\"Wellness\", \"Transportation\"]:\n",
    "    if advisor_name not in loaded_models:\n",
    "        print(f\"üöß Creating fallback CNN model for {advisor_name}...\")\n",
    "        try:\n",
    "            if advisor_name == \"Wellness\":\n",
    "                oracle = CNNWellnessOracle()\n",
    "            else:  # Transportation\n",
    "                oracle = CNNTransportationOracle()\n",
    "            \n",
    "            # Train on a small subset for demonstration\n",
    "            print(f\"   üìö Quick training on subset for {advisor_name}...\")\n",
    "            \n",
    "            # Get some training data\n",
    "            advisor_idx = [\"Wellness\", \"Tax\", \"Transportation\", \"Business\"].index(advisor_name)\n",
    "            train_mask = ~np.isnan(ratings[:, advisor_idx])\n",
    "            \n",
    "            if np.sum(train_mask) > 100:  # Need some data to train\n",
    "                train_grids = grids[train_mask][:1000]  # Use max 1000 for quick training\n",
    "                train_scores = ratings[train_mask, advisor_idx][:1000]\n",
    "                \n",
    "                # Quick training\n",
    "                oracle.fit_model(train_grids, train_scores, epochs=5, verbose=0)\n",
    "                \n",
    "                fallback_models[advisor_name] = oracle\n",
    "                loaded_models[advisor_name] = oracle  # Add to loaded models\n",
    "                \n",
    "                print(f\"   ‚úÖ {advisor_name} fallback model trained!\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Not enough data to train {advisor_name} model\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed to create {advisor_name} fallback: {e}\")\n",
    "\n",
    "if fallback_models:\n",
    "    print(f\"\\nüîÑ Created {len(fallback_models)} fallback CNN models\")\n",
    "    # Update the predictor\n",
    "    oracle_predictor = PreTrainedOraclePredictor(loaded_models)\n",
    "    print(f\"üéØ Updated predictor with {len(loaded_models)} total models\")\n",
    "else:\n",
    "    print(f\"‚úÖ All models loaded successfully, no fallbacks needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828aad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using pre-trained Oracle models (SMALL SUBSET for speed)\n",
    "print(\"üöÄ Generating Oracle predictions using pre-trained models...\")\n",
    "print(\"‚ö° Using small subset of grids for faster processing...\")\n",
    "\n",
    "# Use much smaller subset for faster processing\n",
    "subset_size = 500000  # Reduced from 100,000 to 10,000\n",
    "print(f\"üìä Processing {subset_size:,} grids (subset of {len(grids):,} total)\")\n",
    "\n",
    "# Create random subset for representative sampling\n",
    "np.random.seed(42)  # For reproducibility\n",
    "subset_indices = np.random.choice(len(grids), size=subset_size, replace=False)\n",
    "grids_subset = grids[subset_indices]\n",
    "ratings_subset = ratings[subset_indices]\n",
    "\n",
    "print(f\"‚úÖ Created subset: {grids_subset.shape}\")\n",
    "\n",
    "# Use the pre-trained predictor to generate predictions for subset\n",
    "oracle_prediction_matrix_subset = oracle_predictor.predict_all_advisors(grids_subset)\n",
    "print(f\"Oracle predictions shape: {oracle_prediction_matrix_subset.shape}\")\n",
    "\n",
    "# Create full-size prediction matrix with NaN for non-predicted grids\n",
    "oracle_prediction_matrix = np.full((len(grids), 4), np.nan)\n",
    "oracle_prediction_matrix[subset_indices] = oracle_prediction_matrix_subset\n",
    "\n",
    "# Merge with actual ratings where available (for full dataset)\n",
    "final_oracle_predictions = oracle_prediction_matrix.copy()\n",
    "for advisor_idx in range(4):\n",
    "    mask = ~np.isnan(ratings[:, advisor_idx])\n",
    "    final_oracle_predictions[mask, advisor_idx] = ratings[mask, advisor_idx]\n",
    "\n",
    "print(\"\\n‚úÖ Oracle predictions complete!\")\n",
    "print(f\"Predictions generated for: {subset_size:,} grids\")\n",
    "print(f\"Using actual ratings for: {(~np.isnan(ratings)).sum(axis=0)} samples per advisor\")\n",
    "print(f\"Using Oracle predictions for: {(~np.isnan(oracle_prediction_matrix)).sum(axis=0)} samples per advisor\")\n",
    "\n",
    "# Quick quality check on predicted subset\n",
    "print(f\"\\nüìà Prediction Quality Check (subset):\")\n",
    "for advisor_idx, advisor_name in enumerate(oracle_predictor.advisor_names):\n",
    "    predictions = oracle_prediction_matrix_subset[:, advisor_idx]\n",
    "    print(f\"   ‚Ä¢ {advisor_name:15}: Range [{np.min(predictions):.3f}, {np.max(predictions):.3f}], Mean {np.mean(predictions):.3f}\")\n",
    "\n",
    "# Coverage analysis\n",
    "total_with_data = (~np.isnan(final_oracle_predictions)).sum(axis=0)\n",
    "print(f\"\\nüìä Data Coverage per Advisor:\")\n",
    "for advisor_idx, advisor_name in enumerate(oracle_predictor.advisor_names):\n",
    "    coverage = total_with_data[advisor_idx]\n",
    "    percentage = (coverage / len(grids)) * 100\n",
    "    print(f\"   ‚Ä¢ {advisor_name:15}: {coverage:,} grids ({percentage:.1f}% coverage)\")\n",
    "\n",
    "print(f\"\\nüíæ Predictions ready for analysis and grid optimization!\")\n",
    "print(f\"üí° Note: Using {subset_size:,} grid subset for faster processing\")\n",
    "\n",
    "# Save subset info for reference\n",
    "np.save('oracle_subset_indices.npy', subset_indices)\n",
    "print(f\"üìÅ Saved subset indices to: oracle_subset_indices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e59e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Oracle results (working with subset data)\n",
    "print(\"üîç Analyzing Oracle Results...\")\n",
    "\n",
    "# Since we used a subset, we need to analyze differently\n",
    "# Extract only the grids where we have predictions (non-NaN values)\n",
    "valid_prediction_mask = ~np.isnan(oracle_prediction_matrix).any(axis=1)\n",
    "valid_prediction_indices = np.where(valid_prediction_mask)[0]\n",
    "\n",
    "print(f\"üìä Analysis based on {len(valid_prediction_indices):,} grids with predictions\")\n",
    "\n",
    "# Get predictions for grids that have Oracle predictions\n",
    "subset_predictions = final_oracle_predictions[valid_prediction_mask]\n",
    "subset_grids = grids[valid_prediction_mask]\n",
    "\n",
    "print(f\"Subset predictions shape: {subset_predictions.shape}\")\n",
    "\n",
    "# Calculate min scores for the subset with predictions\n",
    "min_scores_subset = np.min(subset_predictions, axis=1)\n",
    "\n",
    "# Create full arrays with NaN for compatibility\n",
    "min_scores = np.full(len(grids), np.nan)\n",
    "min_scores[valid_prediction_mask] = min_scores_subset\n",
    "\n",
    "valid_mask = np.full(len(grids), False)\n",
    "threshold = 0.75\n",
    "valid_subset_mask = min_scores_subset >= threshold\n",
    "valid_mask[valid_prediction_mask] = valid_subset_mask\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ORACLE PERFORMANCE ANALYSIS (SUBSET)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nüìä VALIDITY ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Total grids with predictions: {len(subset_predictions):,}\")\n",
    "print(f\"   ‚Ä¢ Valid grids (min score ‚â• {threshold}): {np.sum(valid_subset_mask):,}\")\n",
    "print(f\"   ‚Ä¢ Validity rate: {np.sum(valid_subset_mask)/len(subset_predictions)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìà SCORE DISTRIBUTION (SUBSET):\")\n",
    "advisor_names = [\"Wellness\", \"Tax\", \"Transportation\", \"Business\"]\n",
    "for i, advisor in enumerate(advisor_names):\n",
    "    advisor_scores = subset_predictions[:, i]\n",
    "    print(f\"   ‚Ä¢ {advisor:15}: Range [{np.min(advisor_scores):.3f}, {np.max(advisor_scores):.3f}], Mean {np.mean(advisor_scores):.3f}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST PERFORMING GRIDS (SUBSET):\")\n",
    "if np.sum(valid_subset_mask) > 0:\n",
    "    best_indices = np.argsort(min_scores_subset[valid_subset_mask])[-5:]  # Top 5\n",
    "    print(f\"Top 5 minimum scores: {min_scores_subset[valid_subset_mask][best_indices]}\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No grids meet the validity threshold in this subset\")\n",
    "\n",
    "print(f\"\\nüí° Note: Analysis based on {len(subset_predictions):,} grid subset\")\n",
    "print(f\"üìä For full dataset analysis, would need to run predictions on all 500K grids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e43c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top grids from the subset analysis\n",
    "print(f\"\\nüéØ EXTRACTING TOP GRIDS FROM SUBSET:\")\n",
    "\n",
    "# Work with the subset data we analyzed\n",
    "if np.sum(valid_subset_mask) > 0:\n",
    "    # Get indices of valid grids within the subset\n",
    "    valid_subset_indices = np.where(valid_subset_mask)[0]\n",
    "    \n",
    "    # Sort by minimum score (descending)\n",
    "    sorted_indices = np.argsort(min_scores_subset[valid_subset_mask])[::-1]\n",
    "    \n",
    "    # Get top grids (up to 100 or all valid grids, whichever is smaller)\n",
    "    n_top = min(100, len(sorted_indices))\n",
    "    top_subset_indices = valid_subset_indices[sorted_indices[:n_top]]\n",
    "    \n",
    "    # Extract the actual top grids and their predictions\n",
    "    top_grids = subset_grids[top_subset_indices]\n",
    "    top_predictions = subset_predictions[top_subset_indices]\n",
    "    top_min_scores = min_scores_subset[valid_subset_mask][sorted_indices[:n_top]]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Selected {len(top_grids)} grids from subset\")\n",
    "    print(f\"   ‚Ä¢ Min score range: {np.min(top_min_scores):.4f} - {np.max(top_min_scores):.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Score ranges per advisor (top grids):\")\n",
    "    for i, advisor in enumerate(advisor_names):\n",
    "        scores = top_predictions[:, i]\n",
    "        print(f\"     - {advisor:15}: {np.min(scores):.3f} - {np.max(scores):.3f}\")\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Average minimum score: {np.mean(top_min_scores):.4f}\")\n",
    "    \n",
    "    # Save the subset results\n",
    "    np.save('oracle_subset_predictions.npy', subset_predictions)\n",
    "    np.save('oracle_subset_min_scores.npy', min_scores_subset)\n",
    "    np.save('oracle_subset_valid_mask.npy', valid_subset_mask)\n",
    "    np.save('oracle_top_grids_subset.npy', top_grids)\n",
    "    np.save('oracle_top_predictions_subset.npy', top_predictions)\n",
    "    \n",
    "    print(f\"\\nüíæ Saved subset analysis results:\")\n",
    "    print(f\"   ‚Ä¢ oracle_subset_predictions.npy\")\n",
    "    print(f\"   ‚Ä¢ oracle_subset_min_scores.npy\") \n",
    "    print(f\"   ‚Ä¢ oracle_subset_valid_mask.npy\")\n",
    "    print(f\"   ‚Ä¢ oracle_top_grids_subset.npy ({len(top_grids)} grids)\")\n",
    "    print(f\"   ‚Ä¢ oracle_top_predictions_subset.npy\")\n",
    "    \n",
    "    # Visualize some top grids if possible\n",
    "    if len(top_grids) > 0:\n",
    "        print(f\"\\nüìä Top grid analysis:\")\n",
    "        try:\n",
    "            # Try to use the plotting function\n",
    "            os.chdir('2155-Challenge-Problem-2')\n",
    "            plot_n_grids(top_grids[-6:])  # Show top 6 grids\n",
    "            os.chdir('..')\n",
    "            print(\"‚úÖ Displayed top 6 grids\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not display grids: {e}\")\n",
    "            # Show basic stats instead\n",
    "            print(f\"Grid diversity example - Top grid has {len(np.unique(top_grids[-1]))} unique districts\")\n",
    "\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No valid grids found in the subset\")\n",
    "    print(\"   ‚Ä¢ Consider lowering the threshold or using a larger subset\")\n",
    "\n",
    "print(f\"\\nüéâ SUBSET ORACLE ANALYSIS COMPLETE!\")\n",
    "print(f\"üìä Found {np.sum(valid_subset_mask)} valid grids out of {len(subset_predictions):,} analyzed\")\n",
    "print(f\"üí° Validity rate: {np.sum(valid_subset_mask)/len(subset_predictions)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
